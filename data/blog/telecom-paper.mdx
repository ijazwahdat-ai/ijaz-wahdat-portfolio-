---
title: 'Adapting and Optimizing LLaMA-2-7B for Telecom Customer Support in Afghanistan using QLoRA'
date: '2025-12-10'
tags: ['Research', 'LLM', 'Telecom', 'QLoRA']
draft: false
summary: 'The abstract of my research paper submitted to KPU-IJET, detailing the creation of Afghanistanâ€™s first telecom LLM.'
---

# Abstract

The adaptation of Large Language Models (LLMs) to specialized, low-resource domains presents a significant scientific and engineering challenge. This paper addresses these challenges through a comprehensive case study: the development of a domain-specific conversational agent for **telecommunications customer support in Afghanistan**.

We introduce the first publicly available, instruction-following dataset for this domain, comprising **1,750 manually validated English question-answer pairs**. Leveraging **Quantized Low-Rank Adaptation (QLoRA)**, we demonstrate a resource-efficient methodology for fine-tuning the **LLaMA-2-7B** model on a single, consumer-grade GPU.

## Key Findings

*   **Data Scarcity Solved:** Created the AFTel-1750 dataset covering policies from **AWCC**, Salaam, and Roshan.
*   **Performance:** The model achieved a high mean score of **89.0/100 for factual correctness** in a systematic human evaluation by 10 local telecom experts.
*   **Efficiency:** Reduced VRAM usage from ~28GB to ~4GB using 4-bit quantization.

## Conclusion

This research provides a detailed, reproducible blueprint for applying state-of-the-art LLMs in environments with limited resources. It serves as a technical demonstration of fostering local AI capacity and digital sovereignty in Afghanistan.

**[View Full Project Code on GitHub](https://github.com/ijazwahdat-ai/Afghan-Telecom-LLaMA-MCIT)**